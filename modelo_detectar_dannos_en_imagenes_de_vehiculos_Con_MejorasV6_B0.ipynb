{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Damage Detection System - Improved Version V6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import models, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "MIN_SAMPLES_PER_CLASS = 20  # Increased from 15 to 20\n",
    "\n",
    "# Label mappings (unchanged from previous version)\n",
    "label_to_cls_piezas = {...}\n",
    "label_to_cls_danos = {...}\n",
    "label_to_cls_sugerencia = {...}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Dataset Class with Improved Class Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedVehicleDamageDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_path, sep='|')\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Filter rare classes and group some vehicle parts\n",
    "        self._filter_and_group_classes()\n",
    "        \n",
    "    def _filter_and_group_classes(self):\n",
    "        \"\"\"Filter rare classes and group similar vehicle parts\"\"\"\n",
    "        # Group rare vehicle parts into broader categories\n",
    "        def group_parts(part_id):\n",
    "            rare_parts = [4,5,7,8,9,19,20,21,22,23,24,25,27,28,29,30,31,32,33,34,35,36,37,51,52,54,59,60,61,62]\n",
    "            return 99 if part_id in rare_parts else part_id\n",
    "            \n",
    "        self.data['Piezas del Vehículo'] = self.data['Piezas del Vehículo'].apply(group_parts)\n",
    "        \n",
    "        # Filter classes with insufficient samples\n",
    "        for task in ['Tipos de Daño', 'Piezas del Vehículo', 'Sugerencia']:\n",
    "            class_counts = self.data[task].value_counts()\n",
    "            valid_classes = class_counts[class_counts >= MIN_SAMPLES_PER_CLASS].index\n",
    "            self.data = self.data[self.data[task].isin(valid_classes)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        labels = {\n",
    "            'damage': torch.tensor(self.data.iloc[idx, 1] - 1, dtype=torch.long),\n",
    "            'part': torch.tensor(self.data.iloc[idx, 2] - 1, dtype=torch.long),\n",
    "            'suggestion': torch.tensor(self.data.iloc[idx, 3] - 1, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedDamageClassifier(nn.Module):\n",
    "    def __init__(self, num_damage_types, num_parts, num_suggestions):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Simpler backbone\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        # Shared layers with more regularization\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # Task-specific heads with intermediate layers\n",
    "        self.damage_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_damage_types)\n",
    "        )\n",
    "        self.part_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, num_parts)\n",
    "        )\n",
    "        self.suggestion_head = nn.Linear(256, num_suggestions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        shared = self.shared(features)\n",
    "        \n",
    "        return {\n",
    "            'damage': self.damage_head(shared),\n",
    "            'part': self.part_head(shared),\n",
    "            'suggestion': self.suggestion_head(shared)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Training Loop with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_enhanced_model(model, train_loader, val_loader, num_epochs):\n",
    "    # Class-weighted loss functions\n",
    "    damage_weights = get_class_weights(train_dataset, 'damage')\n",
    "    part_weights = get_class_weights(train_dataset, 'part')\n",
    "    suggestion_weights = get_class_weights(train_dataset, 'suggestion')\n",
    "    \n",
    "    criterion = {\n",
    "        'damage': nn.CrossEntropyLoss(weight=damage_weights),\n",
    "        'part': nn.CrossEntropyLoss(weight=part_weights),\n",
    "        'suggestion': nn.CrossEntropyLoss(weight=suggestion_weights)\n",
    "    }\n",
    "    \n",
    "    # Optimizer with weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    \n",
    "    # Learning rate scheduler with warmup\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=1e-3,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = {k: v.to(DEVICE) for k, v in labels.items()}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Weighted multi-task loss\n",
    "            loss = 0.4 * criterion['damage'](outputs['damage'], labels['damage']) + \\\n",
    "                   0.4 * criterion['part'](outputs['part'], labels['part']) + \\\n",
    "                   0.2 * criterion['suggestion'](outputs['suggestion'], labels['suggestion'])\n",
    "            \n",
    "            loss.backward()\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_metrics = evaluate_enhanced_model(model, val_loader)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Loss: {running_loss/len(train_loader):.4f}')\n",
    "        for task in val_metrics:\n",
    "            print(f'{task} Accuracy: {val_metrics[task][\"accuracy\"]:.4f}')\n",
    "            print(f'{task} F1-Score: {val_metrics[task][\"f1\"]:.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Evaluation with Comprehensive Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_enhanced_model(model, loader):\n",
    "    \"\"\"Enhanced evaluation with comprehensive metrics\"\"\"\n",
    "    model.eval()\n",
    "    metrics = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for task in ['damage', 'part', 'suggestion']:\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            for inputs, labels in loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                _, preds = torch.max(outputs[task], 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels[task].cpu().numpy())\n",
    "            \n",
    "            # Calculate multiple metrics\n",
    "            metrics[task] = {\n",
    "                'accuracy': accuracy_score(all_labels, all_preds),\n",
    "                'precision': precision_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
    "                'recall': recall_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
    "                'f1': f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "            }\n",
    "            \n",
    "            # Generate detailed classification report\n",
    "            print(f'\\n=== {task.upper()} Evaluation ===')\n",
    "            print(classification_report(\n",
    "                all_labels, \n",
    "                all_preds, \n",
    "                target_names=[label_to_cls_danos[i+1] if task=='damage' else \n",
    "                             label_to_cls_piezas[i+1] if task=='part' else\n",
    "                             label_to_cls_sugerencia[i+1] for i in np.unique(all_labels)],\n",
    "                zero_division=0\n",
    "            ))\n",
    "            \n",
    "            # Plot confusion matrix\n",
    "            cm = confusion_matrix(all_labels, all_preds)\n",
    "            plt.figure(figsize=(10,8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f'Confusion Matrix - {task}')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.show()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_sample_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     28\u001b[39m val_dataset = EnhancedVehicleDamageDataset(\n\u001b[32m     29\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdata/fotos_siniestros/datasets/val.csv\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     30\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdata/fotos_siniestros/\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     31\u001b[39m     data_transforms[\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     32\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Create data loaders with balanced sampling\u001b[39;00m\n\u001b[32m     35\u001b[39m train_loader = DataLoader(\n\u001b[32m     36\u001b[39m     train_dataset,\n\u001b[32m     37\u001b[39m     batch_size=BATCH_SIZE,\n\u001b[32m     38\u001b[39m     sampler=WeightedRandomSampler(\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m         weights=\u001b[43mget_sample_weights\u001b[49m(train_dataset),\n\u001b[32m     40\u001b[39m         num_samples=\u001b[38;5;28mlen\u001b[39m(train_dataset),\n\u001b[32m     41\u001b[39m         replacement=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     42\u001b[39m     ),\n\u001b[32m     43\u001b[39m     num_workers=\u001b[32m4\u001b[39m\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m val_loader = DataLoader(\n\u001b[32m     47\u001b[39m     val_dataset,\n\u001b[32m     48\u001b[39m     batch_size=BATCH_SIZE,\n\u001b[32m     49\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     50\u001b[39m     num_workers=\u001b[32m4\u001b[39m\n\u001b[32m     51\u001b[39m )\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'get_sample_weights' is not defined"
     ]
    }
   ],
   "source": [
    "# Data transforms with enhanced augmentation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.4, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomPerspective(distortion_scale=0.3, p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EnhancedVehicleDamageDataset(\n",
    "    'data/fotos_siniestros/datasets/train.csv',\n",
    "    'data/fotos_siniestros/',\n",
    "    data_transforms['train']\n",
    ")\n",
    "\n",
    "val_dataset = EnhancedVehicleDamageDataset(\n",
    "    'data/fotos_siniestros/datasets/val.csv',\n",
    "    'data/fotos_siniestros/',\n",
    "    data_transforms['val']\n",
    ")\n",
    "\n",
    "# Create data loaders with balanced sampling\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=WeightedRandomSampler(\n",
    "        weights=get_sample_weights(train_dataset),\n",
    "        num_samples=len(train_dataset),\n",
    "        replacement=True\n",
    "    ),\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = SimplifiedDamageClassifier(\n",
    "    num_damage_types=len(label_to_cls_danos),\n",
    "    num_parts=len(label_to_cls_piezas),\n",
    "    num_suggestions=len(label_to_cls_sugerencia)\n",
    ").to(DEVICE)\n",
    "\n",
    "# Train model\n",
    "trained_model = train_enhanced_model(model, train_loader, val_loader, NUM_EPOCHS)\n",
    "\n",
    "# Save model\n",
    "torch.save(trained_model.state_dict(), 'enhanced_damage_classifier_by_blackboxAI.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
