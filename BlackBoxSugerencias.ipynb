{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import models as torchvision_models\n",
    "from torchvision.models import EfficientNet_B0_Weights\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "## Diccionario para Piezas del Vehículo (completo y corregido)\n",
    "label_to_cls_piezas = {\n",
    "    1: \"Antiniebla delantero derecho\",\n",
    "    2: \"Antiniebla delantero izquierdo\",\n",
    "    3: \"Asiento\",\n",
    "    4: \"Brazo del techo\",\n",
    "    5: \"Brazo transversal\",\n",
    "    6: \"Capó\",\n",
    "    7: \"Cerradura capo\",\n",
    "    8: \"Cerradura maletero\",\n",
    "    9: \"Cerradura puerta\",\n",
    "    10: \"Espejo lateral derecho\",\n",
    "    11: \"Espejo lateral izquierdo\",\n",
    "    12: \"Espejo retrovisor\",\n",
    "    13: \"Faros derecho\",\n",
    "    14: \"Faros izquierdo\",\n",
    "    15: \"Guardabarros delantero derecho\",\n",
    "    16: \"Guardabarros delantero izquierdo\",\n",
    "    17: \"Guardabarros trasero derecho\",\n",
    "    18: \"Guardabarros trasero izquierdo\",\n",
    "    19: \"Limpiaparabrisas\",\n",
    "    20: \"Luz indicadora delantera derecha\",\n",
    "    21: \"Luz indicadora delantera izquierda\",\n",
    "    22: \"Luz indicadora trasera derecha\",\n",
    "    23: \"Luz indicadora trasera izquierda\",\n",
    "    24: \"Luz trasera derecho\",\n",
    "    25: \"Luz trasera izquierdo\",\n",
    "    26: \"Maletero\",\n",
    "    27: \"Manija derecha\",\n",
    "    28: \"Manija izquierda\",\n",
    "    29: \"Marco de la ventana\",\n",
    "    30: \"Marco de las puertas\",\n",
    "    31: \"Matrícula\",\n",
    "    32: \"Moldura capó\",\n",
    "    33: \"Moldura maletro\",\n",
    "    34: \"Moldura puerta delantera derecha\",\n",
    "    35: \"Moldura puerta delantera izquierda\",\n",
    "    36: \"Moldura puerta trasera derecha\",\n",
    "    37: \"Moldura puerta trasera izquierda\",\n",
    "    38: \"Parabrisas delantero\",\n",
    "    39: \"Parabrisas trasero\",\n",
    "    40: \"Parachoques delantero\",\n",
    "    41: \"Parachoques trasero\",\n",
    "    42: \"Poste del techo\",\n",
    "    43: \"Puerta delantera derecha\",\n",
    "    44: \"Puerta delantera izquierda\",\n",
    "    45: \"Puerta trasera derecha\",\n",
    "    46: \"Puerta trasera izquierda\",\n",
    "    47: \"Radiador\",\n",
    "    48: \"Rejilla, parrilla\",\n",
    "    49: \"Rueda\",\n",
    "    50: \"Silenciador, el mofle\",\n",
    "    51: \"Tapa de combustible\",\n",
    "    52: \"Tapa de rueda\",\n",
    "    53: \"Techo\",\n",
    "    54: \"Techo corredizo\",\n",
    "    55: \"Ventana delantera derecha\",\n",
    "    56: \"Ventana delantera izquierda\",\n",
    "    57: \"Ventana trasera derecha\",\n",
    "    58: \"Ventana trasera izquierda\",\n",
    "    59: \"Ventanilla delantera derecha\",\n",
    "    60: \"Ventanilla delantera izquierda\",\n",
    "    61: \"Ventanilla trasera derecha\",\n",
    "    62: \"Ventanilla trasera izquierda\",\n",
    "    63: \"Volante\"\n",
    "}\n",
    "\n",
    "## Diccionario para Tipos de Daño (completo)\n",
    "label_to_cls_danos = {\n",
    "    1: \"Abolladura\",\n",
    "    2: \"Arañazo\",\n",
    "    3: \"Corrosión\",\n",
    "    4: \"Deformación\",\n",
    "    5: \"Desprendimiento\",\n",
    "    6: \"Fractura\",\n",
    "    7: \"Rayón\",\n",
    "    8: \"Rotura\"\n",
    "}\n",
    "\n",
    "## Diccionario para Sugerencia (completo)\n",
    "label_to_cls_sugerencia = {\n",
    "    1: \"Reparar\",\n",
    "    2: \"Reemplazar\"\n",
    "}\n",
    "\n",
    "# =============================================\n",
    "# 1. CONFIGURACIÓN INICIAL\n",
    "# =============================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "batch_size = 32\n",
    "\n",
    "# =============================================\n",
    "# 2. DATASET Y TRANSFORMACIONES MEJORADAS\n",
    "# =============================================\n",
    "class VehiculoDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file, sep='|')\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.class_distribution = self._compute_class_distribution()\n",
    "        \n",
    "    def _compute_class_distribution(self):\n",
    "        return {\n",
    "            'dano': Counter(self.data.iloc[:, 1]),\n",
    "            'pieza': Counter(self.data.iloc[:, 2]),\n",
    "            'sugerencia': Counter(self.data.iloc[:, 3])\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        labels = {\n",
    "            'dano': torch.tensor(self.data.iloc[idx, 1] - 1, dtype=torch.long),\n",
    "            'pieza': torch.tensor(self.data.iloc[idx, 2] - 1, dtype=torch.long),\n",
    "            'sugerencia': torch.tensor(self.data.iloc[idx, 3] - 1, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, labels\n",
    "\n",
    "# Transformaciones mejoradas con aumentación más agresiva\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.4, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomPerspective(distortion_scale=0.3, p=0.5),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.GaussianBlur(kernel_size=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# =============================================\n",
    "# 3. MODELO MEJORADO CON TRANSFER LEARNING EN FASES\n",
    "# =============================================\n",
    "class EnhancedMultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_danos, num_piezas, num_sugerencias):\n",
    "        super().__init__()\n",
    "        # Backbone principal con pesos preentrenados\n",
    "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "        self.base_model = torchvision_models.efficientnet_b0(weights=weights)\n",
    "        \n",
    "        # Congelar todos los parámetros inicialmente\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        in_features = self.base_model.classifier[1].in_features\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "        \n",
    "        # Capas compartidas\n",
    "        self.shared_features = nn.Sequential(\n",
    "            nn.Linear(in_features, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # Cabezales mejorados\n",
    "        # Cabeza para daños\n",
    "        self.dano_head = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, num_danos)\n",
    "        )\n",
    "        \n",
    "        # Cabeza para piezas (mayor capacidad)\n",
    "        self.pieza_head = nn.Sequential(\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(1024, num_piezas)\n",
    "        )\n",
    "        \n",
    "        # Cabeza para sugerencia\n",
    "        self.sugerencia_head = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, num_sugerencias)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for head in [self.dano_head, self.pieza_head, self.sugerencia_head]:\n",
    "            for m in head.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def unfreeze_backbone(self, unfreeze_layers=5):\n",
    "        \"\"\"Descongela capas superiores del backbone\"\"\"\n",
    "        total_layers = len(list(self.base_model.parameters()))\n",
    "        for i, param in enumerate(self.base_model.parameters()):\n",
    "            if i >= total_layers - unfreeze_layers:\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        base_features = self.base_model(x)\n",
    "        shared = self.shared_features(base_features)\n",
    "        \n",
    "        return {\n",
    "            'dano': self.dano_head(shared),\n",
    "            'pieza': self.pieza_head(shared),\n",
    "            'sugerencia': self.sugerencia_head(shared)\n",
    "        }\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# 4. FUNCIONES DE PÉRDIDA MEJORADAS\n",
    "# =============================================\n",
    "# Función para crear samplers balanceados\n",
    "def get_sampler_weights(dataset, task):\n",
    "    # Obtener la columna correcta según la tarea\n",
    "    col_idx = 1 if task == 'dano' else 2 if task == 'pieza' else 3\n",
    "    class_values = dataset.data.iloc[:, col_idx].values\n",
    "    \n",
    "    # Calcular conteos para las clases presentes\n",
    "    unique_classes, counts = np.unique(class_values, return_counts=True)\n",
    "    weights = 1. / counts\n",
    "    \n",
    "    # Crear un mapeo de clase a peso\n",
    "    class_to_weight = {cls: weight for cls, weight in zip(unique_classes, weights)}\n",
    "    \n",
    "    # Asignar pesos a cada muestra\n",
    "    samples_weights = np.array([class_to_weight[cls] for cls in class_values])\n",
    "    \n",
    "    return torch.from_numpy(samples_weights).double()\n",
    "\n",
    "\n",
    "class BalancedFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha='auto', gamma=2.0, num_classes=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        if alpha == 'auto' and num_classes:\n",
    "            self.alpha = torch.ones(num_classes, device=device) / num_classes\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            alpha = self.alpha[targets]\n",
    "            loss = alpha * (1-pt)**self.gamma * ce_loss\n",
    "        else:\n",
    "            loss = (1-pt)**self.gamma * ce_loss\n",
    "            \n",
    "        return loss.mean()\n",
    "\n",
    "def create_criterion(dataset, device):\n",
    "    # Calcular pesos automáticamente basados en clases presentes\n",
    "    def get_weights(task, num_classes):\n",
    "        col_idx = 1 if task == 'dano' else 2 if task == 'pieza' else 3\n",
    "        class_values = dataset.data.iloc[:, col_idx].values\n",
    "        unique_classes, counts = np.unique(class_values, return_counts=True)\n",
    "        weights = torch.zeros(num_classes, device=device)\n",
    "        for cls, count in zip(unique_classes, counts):\n",
    "            weights[cls-1] = 1.0 / count\n",
    "        return weights / weights.sum()\n",
    "    \n",
    "    return {\n",
    "        'dano': BalancedFocalLoss(alpha=get_weights('dano', 8), gamma=2),\n",
    "        'pieza': BalancedFocalLoss(alpha=get_weights('pieza', 63), gamma=2),\n",
    "        'sugerencia': BalancedFocalLoss(alpha=torch.tensor([0.4, 0.6], device=device))\n",
    "    }\n",
    "\n",
    "# =============================================\n",
    "# 5. ENTRENAMIENTO POR ETAPAS (VERSIÓN CORREGIDA)\n",
    "# =============================================\n",
    "def train_phase(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, phase_name, is_reduce_lr=False):\n",
    "    print(f\"\\n=== Fase: {phase_name} ===\")\n",
    "    best_metrics = {'dano': 0, 'pieza': 0, 'sugerencia': 0}\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_metrics = {task: 0 for task in best_metrics}\n",
    "        total_loss = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = {k: v.to(device) for k, v in labels.items()}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            losses = {\n",
    "                task: criterion[task](outputs[task], labels[task]) \n",
    "                for task in best_metrics\n",
    "            }\n",
    "            \n",
    "            task_weights = {\n",
    "                'dano': 0.5 * (1 - best_metrics['dano']),\n",
    "                'pieza': 0.3 * (1 - best_metrics['pieza']),\n",
    "                'sugerencia': 0.2 * (1 - best_metrics['sugerencia'])\n",
    "            }\n",
    "            batch_loss = sum(w * losses[task] for task, w in task_weights.items())\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "            for task in best_metrics:\n",
    "                _, preds = torch.max(outputs[task], 1)\n",
    "                train_metrics[task] += (preds == labels[task]).sum().item()\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_metrics = {task: 0 for task in best_metrics}\n",
    "        val_loss = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = {k: v.to(device) for k, v in labels.items()}\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                batch_size = inputs.size(0)\n",
    "                total_samples += batch_size\n",
    "                \n",
    "                losses = {\n",
    "                    task: criterion[task](outputs[task], labels[task]) \n",
    "                    for task in best_metrics\n",
    "                }\n",
    "                val_loss += sum(losses.values()).item()\n",
    "                \n",
    "                for task in best_metrics:\n",
    "                    _, preds = torch.max(outputs[task], 1)\n",
    "                    val_metrics[task] += (preds == labels[task]).sum().item()\n",
    "        \n",
    "        # Early stopping\n",
    "        val_loss /= len(val_loader)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Actualizar scheduler\n",
    "        if is_reduce_lr:\n",
    "            scheduler.step(0.5*val_metrics['dano'] + 0.3*val_metrics['pieza'] + 0.2*val_metrics['sugerencia'])\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Resultados\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} - Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        for task in best_metrics:\n",
    "            val_acc = val_metrics[task] / total_samples\n",
    "            train_acc = train_metrics[task] / len(train_loader.dataset)\n",
    "            print(f\"{task.capitalize()} - Train: {train_acc:.4f}, Val: {val_acc:.4f}\")\n",
    "            \n",
    "            if val_acc > best_metrics[task]:\n",
    "                best_metrics[task] = val_acc\n",
    "                torch.save(model.state_dict(), f'best_{task}_model.pth')\n",
    "    \n",
    "    return best_metrics\n",
    "\n",
    "# =============================================\n",
    "# 6. PIPELINE COMPLETO (VERSIÓN CORREGIDA)\n",
    "# =============================================\n",
    "# Last Execution 10:04:03 PM\n",
    "# Execution Time 63m 47.5s\n",
    "# Overhead Time 42m 25.7s\n",
    "# Render Times\n",
    "# VS Code Builtin Notebook Output Renderer 3ms\n",
    "\n",
    "def main():\n",
    "    # Cargar datos\n",
    "    image_dir = 'data/fotos_siniestros/'\n",
    "    train_dataset = VehiculoDataset(\n",
    "        csv_file='data/fotos_siniestros/datasets/train.csv',\n",
    "        root_dir=image_dir,\n",
    "        transform=data_transforms['train']\n",
    "    )\n",
    "    val_dataset = VehiculoDataset(\n",
    "        csv_file='data/fotos_siniestros/datasets/val.csv',\n",
    "        root_dir=image_dir,\n",
    "        transform=data_transforms['val']\n",
    "    )\n",
    "    \n",
    "    # Crear samplers balanceados para cada tarea\n",
    "    sampler_dano = WeightedRandomSampler(\n",
    "        get_sampler_weights(train_dataset, 'dano'), \n",
    "        len(train_dataset)\n",
    "    )\n",
    "    sampler_pieza = WeightedRandomSampler(\n",
    "        get_sampler_weights(train_dataset, 'pieza'), \n",
    "        len(train_dataset)\n",
    "    )\n",
    "    \n",
    "    # Usar el sampler para la tarea más desbalanceada (pieza)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler_pieza,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Crear modelo\n",
    "    model = EnhancedMultiTaskModel(8, 63, 2).to(device)\n",
    "    criterion = create_criterion(train_dataset, device)\n",
    "    \n",
    "    # Fase 1: Solo cabezales (10 épocas)\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.shared_features.parameters()},\n",
    "        {'params': model.dano_head.parameters()},\n",
    "        {'params': model.pieza_head.parameters()},\n",
    "        {'params': model.sugerencia_head.parameters()}\n",
    "    ], lr=0.001, weight_decay=1e-3)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=0.01, \n",
    "        steps_per_epoch=len(train_loader), \n",
    "        epochs=10\n",
    "    )\n",
    "    \n",
    "    train_phase(model, train_loader, val_loader, criterion, optimizer, scheduler, 10, \"Entrenamiento inicial (cabezales)\")\n",
    "    \n",
    "    # Fase 2: Descongelar capas superiores (15 épocas)\n",
    "    model.unfreeze_backbone(unfreeze_layers=10)\n",
    "    \n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.base_model.parameters(), 'lr': 0.0001},\n",
    "        {'params': model.shared_features.parameters(), 'lr': 0.0005},\n",
    "        {'params': model.dano_head.parameters(), 'lr': 0.001},\n",
    "        {'params': model.pieza_head.parameters(), 'lr': 0.001},\n",
    "        {'params': model.sugerencia_head.parameters(), 'lr': 0.005}\n",
    "    ], weight_decay=1e-3)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max', \n",
    "        factor=0.5, \n",
    "        patience=2, \n",
    "        verbose=True\n",
    "    )# =============================================\n",
    "# 1. CONFIGURACIÓN INICIAL (MEJORADA)\n",
    "# =============================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "batch_size = 32\n",
    "min_samples_per_class = 15  # Mínimo de muestras por clase\n",
    "    # Fase 3: Fine-tuning completo (15 épocas)\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    train_phase(model, train_loader, val_loader, criterion, optimizer, scheduler, 15, \"Fine-tuning completo\", is_reduce_lr=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# =============================================\n",
    "# EVALUACIÓN FINAL Y MATRICES DE CONFUSIÓN\n",
    "# =============================================\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title='Matriz de Confusión', cmap=plt.cm.Blues, figsize=(12, 10)):\n",
    "    \"\"\"Función mejorada para visualizar matrices de confusión\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, \n",
    "                xticklabels=classes, yticklabels=classes,\n",
    "                cbar=False, linewidths=0.5, linecolor='gray')\n",
    "    \n",
    "    plt.title(title, fontsize=14, pad=20)\n",
    "    plt.xlabel('Predicción', fontsize=12)\n",
    "    plt.ylabel('Real', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(rotation=0, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar la figura\n",
    "    filename = title.lower().replace(' ', '_') + '.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_task(model, loader, task_name, label_dict):\n",
    "    \"\"\"Evalúa un modelo en una tarea específica y genera métricas\"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs[task_name], 1)\n",
    "            y_true.extend(labels[task_name].cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Convertir a arrays numpy\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Filtrar solo clases presentes\n",
    "    present_labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    class_names = [label_dict[label+1] for label in present_labels]\n",
    "    \n",
    "    # Manejo especial para piezas (muchas clases)\n",
    "    if task_name == 'pieza' and len(present_labels) > 20:\n",
    "        label_counts = {label: np.sum(y_true == label) for label in present_labels}\n",
    "        top_labels = sorted(present_labels, key=lambda x: label_counts[x], reverse=True)[:20]\n",
    "        mask = np.isin(y_true, top_labels)\n",
    "        y_true_filtered = y_true[mask]\n",
    "        y_pred_filtered = y_pred[mask]\n",
    "        class_names = [label_dict[label+1] for label in top_labels]\n",
    "    else:\n",
    "        y_true_filtered = y_true\n",
    "        y_pred_filtered = y_pred\n",
    "    \n",
    "    print(f\"\\n=== Resultados para {task_name.capitalize()} ===\")\n",
    "    \n",
    "    # Obtener las etiquetas presentes en los datos\n",
    "    labels_present = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    target_names = [label_dict[label+1] for label in labels_present]\n",
    "    \n",
    "    print(classification_report(\n",
    "        y_true, \n",
    "        y_pred, \n",
    "        labels=labels_present,\n",
    "        target_names=target_names,\n",
    "        zero_division=0,\n",
    "        digits=4\n",
    "    ))\n",
    "    \n",
    "    plot_confusion_matrix(\n",
    "        y_true_filtered, \n",
    "        y_pred_filtered, \n",
    "        classes=class_names,\n",
    "        title=f'Matriz de Confusión - {task_name.capitalize()}',\n",
    "        figsize=(15, 12) if task_name == 'pieza' else (10, 8)\n",
    "    )\n",
    "\n",
    "# Diccionarios de etiquetas\n",
    "label_dicts = {\n",
    "    'dano': label_to_cls_danos,\n",
    "    'pieza': label_to_cls_piezas,\n",
    "    'sugerencia': label_to_cls_sugerencia\n",
    "}\n",
    "\n",
    "# =============================================\n",
    "# CARGA Y EVALUACIÓN DE MODELOS ENTRENADOS\n",
    "# =============================================\n",
    "def load_and_evaluate_models():\n",
    "    \"\"\"Carga los modelos entrenados y genera las matrices de confusión\"\"\"\n",
    "    # Cargar los mejores modelos para cada tarea\n",
    "    tasks = ['dano', 'pieza', 'sugerencia']\n",
    "    model_instances = {}  # Cambio de nombre para evitar conflicto\n",
    "    \n",
    "    for task in tasks:\n",
    "        model_path = f'best_{task}_model.pth'\n",
    "        if os.path.exists(model_path):\n",
    "            # Crear nueva instancia del modelo\n",
    "            model = EnhancedMultiTaskModel(8, 63, 2).to(device)\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            model_instances[task] = model\n",
    "            print(f\"Modelo para {task} cargado exitosamente\")\n",
    "        else:\n",
    "            print(f\"Advertencia: No se encontró el modelo para {task} en {model_path}\")\n",
    "    \n",
    "    # Verificar si val_loader está definido\n",
    "    if 'val_loader' not in locals():\n",
    "        val_dataset = VehiculoDataset(\n",
    "            csv_file='data/fotos_siniestros/datasets/val.csv',\n",
    "            root_dir='data/fotos_siniestros/',\n",
    "            transform=data_transforms['val']\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    # Evaluar cada tarea\n",
    "    for task in tasks:\n",
    "        if task in model_instances:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Evaluando modelo para tarea: {task.upper()}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            evaluate_task(model_instances[task], val_loader, task, label_dicts[task])\n",
    "\n",
    "# Ejecutar la evaluación\n",
    "if __name__ == \"__main__\":\n",
    "    load_and_evaluate_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================\n",
    "## Salida\n",
    "==================================================\n",
    "\n",
    "=== Fase: Entrenamiento inicial (cabezales) ===\n",
    "\n",
    "Epoch 1/10 - Loss: 0.8768, Val Loss: 2.2638\n",
    "Dano - Train: 0.2232, Val: 0.1760\n",
    "Pieza - Train: 0.1301, Val: 0.0146\n",
    "Sugerencia - Train: 0.5590, Val: 0.5561\n",
    "\n",
    "Epoch 2/10 - Loss: 0.2723, Val Loss: 1.8339\n",
    "Dano - Train: 0.2518, Val: 0.1839\n",
    "Pieza - Train: 0.2279, Val: 0.0056\n",
    "Sugerencia - Train: 0.5701, Val: 0.5830\n",
    "\n",
    "Epoch 3/10 - Loss: 0.2153, Val Loss: 1.6102\n",
    "Dano - Train: 0.2846, Val: 0.2164\n",
    "Pieza - Train: 0.2523, Val: 0.0101\n",
    "Sugerencia - Train: 0.5747, Val: 0.5426\n",
    "\n",
    "Epoch 4/10 - Loss: 0.1784, Val Loss: 1.1537\n",
    "Dano - Train: 0.2966, Val: 0.2052\n",
    "Pieza - Train: 0.2583, Val: 0.0101\n",
    "Sugerencia - Train: 0.5909, Val: 0.5303\n",
    "\n",
    "Epoch 5/10 - Loss: 0.1600, Val Loss: 1.1449\n",
    "Dano - Train: 0.3007, Val: 0.2646\n",
    "...\n",
    "Sugerencia - Train: 0.6042, Val: 0.5235\n",
    "Early stopping at epoch 10\n",
    "\n",
    "=== Fase: Fine-tuning parcial ===\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n",
    "/data/Python/VehiculosVerificationDeDannosEtiquetas/.venv/lib64/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
    "  warnings.warn(\n",
    "\n",
    "Epoch 1/15 - Loss: 0.6888, Val Loss: 0.9349\n",
    "Dano - Train: 0.3270, Val: 0.2096\n",
    "Pieza - Train: 0.2721, Val: 0.0022\n",
    "Sugerencia - Train: 0.5577, Val: 0.5235\n",
    "\n",
    "Epoch 2/15 - Loss: 0.0822, Val Loss: 0.2629\n",
    "Dano - Train: 0.3021, Val: 0.2646\n",
    "Pieza - Train: 0.2823, Val: 0.0090\n",
    "Sugerencia - Train: 0.5526, Val: 0.5235\n",
    "\n",
    "Epoch 3/15 - Loss: 0.0482, Val Loss: 0.2140\n",
    "Dano - Train: 0.3247, Val: 0.2993\n",
    "Pieza - Train: 0.3003, Val: 0.0247\n",
    "Sugerencia - Train: 0.5756, Val: 0.5639\n",
    "\n",
    "Epoch 4/15 - Loss: 0.0413, Val Loss: 0.2403\n",
    "Dano - Train: 0.3141, Val: 0.3128\n",
    "Pieza - Train: 0.3367, Val: 0.0067\n",
    "Sugerencia - Train: 0.5812, Val: 0.4843\n",
    "\n",
    "Epoch 5/15 - Loss: 0.0335, Val Loss: 0.3034\n",
    "Dano - Train: 0.3455, Val: 0.2534\n",
    "Pieza - Train: 0.3372, Val: 0.0112\n",
    "Sugerencia - Train: 0.5530, Val: 0.6132\n",
    "...\n",
    "Dano - Train: 0.3589, Val: 0.2960\n",
    "Pieza - Train: 0.4327, Val: 0.0348\n",
    "Sugerencia - Train: 0.6135, Val: 0.5774\n",
    "Early stopping at epoch 9\n",
    ")\n",
    "\n",
    "Modelo para dano cargado exitosamente\n",
    "Modelo para pieza cargado exitosamente\n",
    "Modelo para sugerencia cargado exitosamente\n",
    "\n",
    "==================================================\n",
    "Evaluando modelo para tarea: DANO\n",
    "==================================================\n",
    "\n",
    "=== Resultados para Dano ===\n",
    "\n",
    "                    precision    recall  f1-score   support\n",
    "\n",
    "        Abolladura     0.3395    0.4015    0.3679       274\n",
    "            Arañazo     0.3107    0.2286    0.2634       140\n",
    "        Deformación     0.2000    0.0408    0.0678        49\n",
    "    Desprendimiento     0.1746    0.2178    0.1938       101\n",
    "          Fractura     0.0460    0.1212    0.0667        33\n",
    "              Rayón     0.1000    0.1333    0.1143        15\n",
    "            Rotura     0.4910    0.3893    0.4343       280\n",
    "\n",
    "          accuracy                         0.3150       892\n",
    "          macro avg     0.2374    0.2189    0.2154       892\n",
    "      weighted avg     0.3413    0.3150    0.3207       892\n",
    "\n",
    "==================================================\n",
    "Evaluando modelo para tarea: PIEZA\n",
    "==================================================\n",
    "\n",
    "=== Resultados para Pieza ===\n",
    "\n",
    "                                        precision    recall  f1-score   support\n",
    "\n",
    "          Antiniebla delantero derecho     0.0000    0.0000    0.0000         0\n",
    "        Antiniebla delantero izquierdo     0.0000    0.0000    0.0000         6\n",
    "                      Brazo del techo     0.0000    0.0000    0.0000         4\n",
    "                                  Capó     0.0000    0.0000    0.0000        55\n",
    "                Espejo lateral derecho     0.0000    0.0000    0.0000         6\n",
    "              Espejo lateral izquierdo     0.0000    0.0000    0.0000         6\n",
    "                        Faros derecho     0.0000    0.0000    0.0000        36\n",
    "                      Faros izquierdo     0.0000    0.0000    0.0000        50\n",
    "        Guardabarros delantero derecho     0.0000    0.0000    0.0000        58\n",
    "      Guardabarros delantero izquierdo     0.0000    0.0000    0.0000        77\n",
    "          Guardabarros trasero derecho     0.0000    0.0000    0.0000        19\n",
    "        Guardabarros trasero izquierdo     0.0588    0.0357    0.0444        28\n",
    "                      Limpiaparabrisas     0.0000    0.0000    0.0000         0\n",
    "      Luz indicadora delantera derecha     0.0769    1.0000    0.1429         6\n",
    "    Luz indicadora delantera izquierda     0.0000    0.0000    0.0000         8\n",
    "      Luz indicadora trasera izquierda     0.0000    0.0000    0.0000         0\n",
    "                  Luz trasera derecho     0.0000    0.0000    0.0000        20\n",
    "    ...\n",
    "                              accuracy                         0.0504       892\n",
    "                            macro avg     0.0340    0.1039    0.0402       892\n",
    "                          weighted avg     0.0205    0.0504    0.0234       892\n",
    "\n",
    "==================================================\n",
    "Evaluando modelo para tarea: SUGERENCIA\n",
    "==================================================\n",
    "\n",
    "=== Resultados para Sugerencia ===\n",
    "\n",
    "                  precision    recall  f1-score   support\n",
    "\n",
    "        Reparar     0.6245    0.9006    0.7376       543\n",
    "      Reemplazar     0.5046    0.1576    0.2402       349\n",
    "\n",
    "        accuracy                         0.6099       892\n",
    "      macro avg     0.5646    0.5291    0.4889       892\n",
    "    weighted avg     0.5776    0.6099    0.5430       892"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Areas for Improvement\n",
    "### Model Architecture:\n",
    "\n",
    "1. Consider experimenting with different architectures or fine-tuning strategies.\n",
    "Evaluate the effectiveness of the current layers and dropout rates.\n",
    "Data Augmentation:\n",
    "\n",
    "2. Review the data augmentation techniques used and consider adding or modifying them to improve model robustness.\n",
    "Training Process:\n",
    "\n",
    "3. Analyze the learning rate scheduling and optimizer settings.\n",
    "Implement early stopping based on validation metrics to prevent overfitting.\n",
    "Evaluation Metrics:\n",
    "\n",
    "4. Improve the evaluation metrics to provide more insights into model performance, especially for imbalanced classes.\n",
    "Logging and Visualization:\n",
    "\n",
    "5. Enhance logging during training to capture more detailed metrics.\n",
    "Improve visualization of training and validation losses.\n",
    "Plan for Improvements\n",
    "Model Architecture:\n",
    "\n",
    "6. Experiment with additional layers or different activation functions.\n",
    "Adjust dropout rates based on validation performance.\n",
    "Data Augmentation:\n",
    "\n",
    "7. Add more aggressive augmentations or consider using techniques like CutMix or MixUp.\n",
    "Training Process:\n",
    "\n",
    "8. Implement a more sophisticated learning rate scheduler.\n",
    "Add early stopping based on validation loss.\n",
    "Evaluation Metrics:\n",
    "\n",
    "9. Include additional metrics such as F1 score, precision, and recall for each class.\n",
    "Visualize confusion matrices for better insights.\n",
    "Logging and Visualization:\n",
    "\n",
    "10. Use TensorBoard or similar tools for better visualization of training metrics.\n",
    "\n",
    "### Steps to Implement Improvements\n",
    "1. Model Architecture:\n",
    "    - Experiment with additional layers or different activation functions.\n",
    "    - Adjust dropout rates based on validation performance.\n",
    "\n",
    "2. Data Augmentation:\n",
    "    - Add more aggressive augmentations or consider using techniques like CutMix or MixUp.\n",
    "\n",
    "3. Training Process:\n",
    "    - Implement a more sophisticated learning rate scheduler.\n",
    "    - Add early stopping based on validation loss.\n",
    "\n",
    "4. Evaluation Metrics:\n",
    "    - Include additional metrics such as F1 score, precision, and recall for each class.\n",
    "    - Visualize confusion matrices for better insights.\n",
    "\n",
    "5. Logging and Visualization:\n",
    "    - Use TensorBoard or similar tools for better visualization of training metrics.\n",
    "\n",
    "### Key Observations:\n",
    "1. Model Architecture: The current model uses a ResNet50 backbone with separate heads for damage type, vehicle part, and suggestion.\n",
    "2. Data Augmentation: The notebook includes data augmentation techniques, but there may be room for improvement.\n",
    "3. Training Process: The training process is defined, but enhancements can be made in learning rate scheduling and early stopping.\n",
    "4. Evaluation Metrics: The evaluation metrics are currently limited; additional metrics like F1 score, precision, and recall could provide better insights.\n",
    "5. Logging and Visualization: There is a need for enhanced logging and visualization of training metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
